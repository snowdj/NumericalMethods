% $Header: /cvsroot/latex-beamer/latex-beamer/solutions/conference-talks/conference-ornate-20min.en.tex,v 1.6 2004/10/07 20:53:08 tantau Exp $

\documentclass{beamer}

\mode<presentation>
{
  \usetheme{Hawke}
  % or ...

  \setbeamercovered{transparent}
  % or whatever (possibly just delete it)
}


\usepackage[english]{babel}
% or whatever

\usepackage[latin1]{inputenc}
% or whatever

\usepackage{times}
\usepackage[T1]{fontenc}

\usepackage{multimedia}


%%%%%%
% My Commands
%%%%%%

\newcommand{\bb}{{\boldsymbol{b}}}
\newcommand{\bx}{{\boldsymbol{x}}}
\newcommand{\by}{{\boldsymbol{y}}}
\newcommand{\bfm}[1]{{\boldsymbol{#1}}}

%%%%

\title[Lecture 16] % (optional, use only with long paper titles)
{Lecture 16 - Initial Value Problems}


\author[I. Hawke] % (optional, use only with lots of authors)
{I.~Hawke}

\institute[University of Southampton] % (optional, but mostly needed)
{
%  \inst{1}%
  School of Mathematics, \\
  University of Southampton, UK
}

\date[Semester 1] % (optional, should be abbreviation of conference name)
{MATH3018/6141, Semester 1}

\subject{Numerical methods}
% This is only inserted into the PDF information catalog. Can be left
% out.


\pgfdeclareimage[height=0.5cm]{university-logo}{mathematics_7469}
\logo{\pgfuseimage{university-logo}}


\AtBeginSection[]
{
  \begin{frame}<beamer>
    \frametitle{Outline}
    \tableofcontents[currentsection]
  \end{frame}
}



\begin{document}

\begin{frame}
  \titlepage
\end{frame}

\section{Predictor-Corrector methods}

\subsection{Predictor-Corrector methods}

\begin{frame}
  \frametitle{Geometrical interpretation of Euler's method}

  Considering IVPs in the form
  \begin{equation*}
    \by'(x) = \bfm{f}(x, \by(x)).
  \end{equation*}

  The simple (first order accurate, explicit) Euler method is
  \begin{equation*}
    \by_{n+1} = \by_n + h \bfm{f}(x_n, \by_n).
  \end{equation*} \pause

  Euler's method is not sufficiently accurate for practical use. The
  geometrical interpretation is that the slope ($\by'$) over the
  interval $[x_n, x_{n+1}]$ is approximated by the slope at the
  \emph{beginning} of the interval,
  \begin{equation*}
    \by' \simeq \by'(x_n).
  \end{equation*} \pause

  Try a better approximation to the slope for a more accurate result.

\end{frame}

\begin{frame}
  \frametitle{Predicting and correcting}

  \begin{overlayarea}{\textwidth}{0.8\textheight}
    \only<1-3|handout:1>
    {
      Still using forward differencing for $\by'$, an ``average''
      slope is
      \begin{equation*}
        \frac{\by_{n+1} - \by_n}{h} = \frac{\bfm{f}(x_n, \by_n) +
          \bfm{f}(x_{n+1}, \by_{n+1})}{2}.
      \end{equation*}
    }
    \only<2-3|handout:1>
    {
      Rearrange: gives the unknown $\by_{n+1}$ as
      \begin{equation*}
        \by_{n+1} = \by_n + \frac{h}{2} \left( \bfm{f}(x_n, \by_n) +
          \bfm{f}(x_{n+1}, \by_{n+1}) \right).
      \end{equation*}
    }
    \only<3|handout:1>
    {
      But we need $\by_{n+1}$ to compute the right-hand-side.
    }
    \only<4-|handout:2>
    {
      \begin{equation*}
        \by_{n+1} = \by_n + \frac{h}{2} \left( \bfm{f}(x_n, \by_n) +
          \bfm{f}(x_{n+1}, \by_{n+1}) \right).
      \end{equation*}

      \vspace{1ex}

      Instead compute a \emph{guess} for $\by_{n+1}$, and use this
      guess when required. This is the \emph{predictor} step:
      \begin{equation*}
        \by_{n+1}^{(p)} = \by_n + h \bfm{f}(x_n, \by_n)
      \end{equation*}
      if we use Euler's method for the prediction.
    }
    \only<5-|handout:2>
    {

      \vspace{1ex}
      Using this, apply the \emph{corrector} step
      \begin{equation*}
        \by_{n+1} = \by_n + \frac{h}{2} \left( \bfm{f}(x_n, \by_n) +
          \bfm{f}(x_{n+1}, \by_{n+1}^{(p)}) \right),
      \end{equation*}
      which describes the (Euler) \emph{predictor-corrector} method.
    }
    \only<6|handout:2>
    {

      \vspace{1ex}
      The \emph{local} error is $\propto {\cal O}(h^3)$.
    }
  \end{overlayarea}

\end{frame}

\begin{frame}
  \frametitle{Example}


  Apply the Euler predictor-corrector method to
  \begin{equation*}
    y'(x) = - \sin(x), \quad y(0) = 1
  \end{equation*}
  to integrate up to $x = 0.5$. With $h = 0.1$:
  \begin{center}
    \begin{tabular}{c|c c c c c c}
      $n$ & $x_n$ & $y_n$ & $f(x_n, y_n)$ & $y_{n+1}^{(p)}$ & $f(x_{n+1},
      y_{n+1}^{(p)})$ & $\cos(x_n)$ \\
      \hline
      0 & 0.0 & 1.000 &  0.000 & 1.000 & -0.100 & 1.000 \\
      1 & 0.1 & 0.995 & -0.100 & 0.985 & -0.199 & 0.995 \\
      2 & 0.2 & 0.980 & -0.199 & 0.960 & -0.296 & 0.980 \\
      3 & 0.3 & 0.955 & -0.296 & 0.926 & -0.389 & 0.955 \\
      4 & 0.4 & 0.921 & -0.389 & 0.882 & -0.479 & 0.921 \\
      5 & 0.5 & 0.878 &        &       &        & 0.878
    \end{tabular}
  \end{center} \pause
  The error is $0.1\%$, not visible at this precision. With $h = 0.01$
  then the error is $0.001\%$, showing second order convergence.

\end{frame}


\begin{frame}
  \frametitle{Example: 2}


  Consider the system
  \begin{equation*}
    \left\{
      \begin{aligned}
        \dot{x} & = -y \\ \dot{y} & = x
      \end{aligned} \right., \quad x(0) = 1, \, \, y(0) = 0.
  \end{equation*}
  In polar coordinates this is $\dot{r} = 0$, $\dot{\phi} = 1$.
  \begin{columns}
    \begin{column}{0.5\textwidth}
      \begin{overlayarea}{\textwidth}{0.4\textheight}
        \only<2-3|handout:1>
        {
          Use the predictor-corrector method with $h=0.1$. At $t=500$
          the result matches the correct answer to the eye.
        }
        \only<3|handout:1>
        {

          \vspace{1ex}
          The growth of the radius makes the errors visible.
        }
        \only<4-5|handout:2>
        {
          Use the predictor-corrector method with $h=0.01$. At $t=500$
          the result matches the correct answer to the eye.
        }
        \only<5|handout:2>
        {

          \vspace{1ex}
          Looking at the growth of the radius makes the errors
          visible even now.
        }
      \end{overlayarea}
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{overlayarea}{\textwidth}{0.6\textheight}
        \only<2|handout:0>
        {
          \begin{center}
            \includegraphics[height=0.5\textheight]{figures/EulerPC1}
          \end{center}
        }
        \only<3|handout:1>
        {
          \begin{center}
            \includegraphics[height=0.5\textheight]{figures/EulerPC_rad1}
          \end{center}
        }
        \only<4|handout:0>
        {
          \begin{center}
            \includegraphics[height=0.5\textheight]{figures/EulerPC2}
          \end{center}
        }
        \only<5|handout:2>
        {
          \begin{center}
            \includegraphics[height=0.5\textheight]{figures/EulerPC_rad2}
          \end{center}
        }
      \end{overlayarea}
    \end{column}
  \end{columns}

\end{frame}


\section{Errors}

\subsection{Errors}

\begin{frame}
  \frametitle{Errors}


  \begin{columns}
    \begin{column}{0.5\textwidth}
      With quadrature using e.g.\ Simpson's rule the \emph{local}
      error on any subinterval gave
      \begin{equation*}
        |{\cal E}_{\text{global}}| \le \sum |{\cal E}_{\text{local}}|.
      \end{equation*} \pause

      For an IVP we still have the \emph{local} and \emph{global}
      errors.  But steps are not independent so the error bound is
      different.
    \end{column}
    \begin{column}{0.5\textwidth}
      \begin{center}
        \includegraphics<1|handout:0>[width=\textwidth]{figures/simpson2}
        \includegraphics<2>[width=\textwidth]{figures/Errors1}
      \end{center}
    \end{column}
  \end{columns}

\end{frame}

\begin{frame}
  \frametitle{Truncation vs.\ roundoff}

  All error (local and global) has two pieces: \emph{truncation} and
  \emph{roundoff} error.
  \begin{enumerate}
  \item Roundoff: inherent in the floating point
    representation. Usually extremely small, it cannot be controlled.
    \pause
  \item Truncation: the result of replacing the continuous
    differential equation with a discrete algorithm. Usually
    dominant. Typically depends on ``step size'' $h$ which can be
    controlled.
  \end{enumerate} \pause

  \vspace{1ex}

  The \emph{total error} (to minimize) is the sum of the global
  truncation and global roundoff errors.

  \vspace{1ex}

  We bound the global truncation error only as we expect roundoff
  error to be negligible.

\end{frame}

\begin{frame}
  \frametitle{Error accumulation}

  \begin{equation*}
    |{\cal E}_{\text{global, truncation}}| \ne \sum |{\cal
      E}_{\text{local, truncation}}|.
  \end{equation*}
  Each step depends on the previous answer: \emph{total} previous
  error changes the initial data for each step. \pause So first bound
  the effect of this uncertainty in the initial data:

  \vspace{1ex}

  {\bf Theorem}: For the IVP
  \begin{equation*}
    y' = f(x, y), \quad y(0) = s, \quad 0 \le x \le X > 0,
  \end{equation*}
  the variation of the solution dependent on the initial data is
  bounded by
  \begin{equation*}
    \left| \frac{\partial y}{\partial s} \right| \leq e^{\lambda x}
  \end{equation*}
  where $\partial_y f \leq \lambda$ in $0 \le x \le X$.

\end{frame}

\begin{frame}
  \frametitle{Error accumulation: 2}

  If the IVP
  \begin{equation*}
    y' = f(x, y), \quad 0 \le x \le X > 0,
  \end{equation*}
  is solved with \emph{different} initial data $y(0) = s$, $y(0) = s +
  \delta$, then above implies that two initially ``close'' solutions
  will behave as
  \begin{equation*}
    | y(x, s) - y(x, s + \delta)| \leq |\delta| e^{\lambda x}.
  \end{equation*}
  \pause

  \vspace{1ex}

  At best the error growth is proportional to the exponential of the
  step length \emph{and} proportional to the initial error. \pause

  \vspace{1ex}

  Put this all together by summing over all steps.

\end{frame}

\begin{frame}
  \frametitle{Error accumulation: 3}

  {\bf Global Error Theorem}: If $|{\cal E}_{\text{local,
      truncation}}| \le \delta \propto h^{p+1}$, the global truncation
  error after $n$ steps is bounded by
  \begin{equation*}
    \delta \frac{1 - e^{n \lambda h}}{1 - e^{\lambda h}}.
  \end{equation*} \pause

  \vspace{1ex}

  By induction compute the ``initial'' error for each step; the
  previous theorem bounds $\delta_n$ in terms of $\lambda$ and
  $h$. Result follows from a geometric progression.  \pause

  \vspace{1ex}

  To use this theorem \emph{assume} that $\lambda h \ll 1$. We have $n
  h = X = (b - a)$. Taylor expanding the error bound we have
  \begin{equation*}
    |{\cal E}_{\text{Global}}|  \leq \left| \delta \frac{(1 -
      e^{\lambda(b-a)})}{-\lambda h - {\cal O}(h^2)} \right|
    \leq {\cal O}(h^p) \left| 1 - e^{\lambda(b-a)} \right|.
  \end{equation*}

\end{frame}

\section{Summary}

\subsection{Summary}

\begin{frame}
  \frametitle{Summary}

  \begin{itemize}
  \item The errors at each step in an IVP solver are not independent.
    Hence the total error is not bounded by the sum of the local
    errors.
  \item If the local error is ${\cal O}(h^{n+1})$ then the global
    error is ${\cal O}(h^{n})$. Such a method is said to be of
    \emph{order $n$}.
  \item Euler's method has local error $\propto h^2$, hence global
    error $\propto h$.
  \item The Euler predictor-corrector method has local error $\propto
    h^3$, hence global error $\propto h^2$.
  \end{itemize}

\end{frame}

\end{document}



%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
