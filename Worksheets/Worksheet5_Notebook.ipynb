{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worksheet 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbconvert": {
     "hide_code": true
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain when multistep methods such as Adams-Bashforth are useful and when multistage methods such as RK methods are better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "Multistep methods are more computationally efficient (fewer function evaluations) and more accurate than multistage methods. However, they are not self-starting, difficult to adapt to use variable step sizes, and the theory to show that they are stable and convergent is more complex. They are most useful when efficiency is the primary concern and the system is sufficiently well controlled that equally spaced steps can be taken.\n",
    "\n",
    "In other situations, as discussed on worksheet 4, the self-starting simplicity combined with adaptive stepping means that multistage methods are preferrable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the coefficients of the AB3 algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "For Adams-Bashforth methods we have\n",
    "$$\n",
    "  y_{n+1} = y_n + b_{k−1} f_n + b_{k−2} f_{n−1} + \\dots + b_0 f_{n+1−k}.\n",
    "$$\n",
    "\n",
    "Here we have $k = 3$ and so we have\n",
    "$$\n",
    "  y_{n+1} = y_n + h \\left[ b_2 f_n + b_1 f_{n−1} + b_0 f_{n−2} \\right] .\n",
    "$$\n",
    "\n",
    "We want to ensure that this gives an exact approximation of the integral form for polynomials of\n",
    "order s = $0, \\dots, 2$. That is, we want\n",
    "$$\n",
    "  \\int^{x_{n+1}}_{x_n} p_s (x) = h \\left[ b_2 p_s (x_n) + b_1 p_s (x_{n−1}) + b_0 p_s (x_{n−2}) \\right].\n",
    "$$\n",
    "\n",
    "For simplicity, and without loss of generality, we set $x_n = 0$, and use the polynomials\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  p_0(x) & = 1, \\\\\n",
    "  p_1(x) & = x, \\\\\n",
    "  p_2(x) & = x ( x + h )\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We then see that we get \n",
    "$$\n",
    "\\begin{aligned}\n",
    "  s & = 0: &   \\int_0^h 1 & = h \\left[ b_2 \\times 1 + b_1 \\times 1 + b_0 \\times 1 \\right] \\\\\n",
    "  \\implies &&           1 & = b_2 + b_1 + b_0. \\\\\n",
    "  s & = 1: &   \\int_0^h x & = h \\left[ b_2 \\times 0 + b_1 \\times (-h) + b_0 \\times (-2 h) \\right] \\\\\n",
    "  \\implies && \\frac{1}{2} & = -b_1 - 2 b_0. \\\\\n",
    "  s & = 2: & \\int_0^h x ( x + h ) & = h \\left[ b_2 \\times 0 + b_1 \\times 0 + b_0 \\times (-2 h) (-h) \\right] \\\\\n",
    "  \\implies && \\frac{5}{6} & = 2 b_0. \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "By back-substitution we find\n",
    "$$\n",
    "  b_0 = \\frac{5}{12}, \\quad b_1 = -\\frac{4}{3}, \\quad b_2 = \\frac{23}{12}.\n",
    "$$\n",
    "\n",
    "This means the algorithm is\n",
    "$$\n",
    "  y_{n+1} = y_n + \\frac{h}{12} \\left[ 23 f_n − 16 f_{n−1} + 5 f_{n−2} \\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the meaning of stability, consistency and convergence when applied to numerical methods for IVPs. State the theorem connecting these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "*Stability*: The numerical solution is bounded at all iterations over a finite interval. I.e., if the true solution is $y(x)$ and $x \\in [0, X]$ with $X$ finite, and we use $N + 1$ steps with $x_0 = 0$ and $x_N = X$, then $|y_i|$ is finite for all $i = 0, 1, \\dots , N$, irrespective of the value of $N$.\n",
    "\n",
    "*Consistency*: The numerical method is a faithful representation of the differential equation to lowest order in $h$. That is, if you Taylor expand the numerical difference scheme and let $h \\to 0$ you recover the original differential equation independent of the limiting process.\n",
    "\n",
    "*Convergence*: If $y(x)$ is the exact solution and $y(x; h)$ the numerical solution using step size $h$, in the limit as $h \\to 0$ the numerical solution is the exact solution:\n",
    "$$\n",
    " \\lim_{h \\to 0} y(x; h) = y(x).\n",
    "$$\n",
    "\n",
    "The theorem states that consistency and stability are equivalent to convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the stability polynomial and your results above, check the order of accuracy and the stability of the 3 step Adams-Bashforth method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "The coefficients of AB3 in the standard $k$-step formula notation are\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  a_3 & = 1 & a_2 & = -1 & a_1 & = 0 & a_0 & = 0 \\\\\n",
    "  b_3 & = 0 & b_2 & = \\frac{23}{12} & b_1 & = -\\frac{4}{3} & b_0 & = \\frac{5}{12}.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Therefore the stability polynomial is\n",
    "$$\n",
    "  p(z) = z^3 - z^2\n",
    "$$\n",
    "with derivative\n",
    "$$\n",
    "  p'(z) = 3 z^2 - 2 z\n",
    "$$\n",
    "and the other required polynomial is\n",
    "$$\n",
    "  q(z) = \\frac{1}{12} \\left( 23 z^2 - 16 z + 5 \\right).\n",
    "$$\n",
    "\n",
    "To check consistency we need that $p(1) = 0$ and $p'(1) = q(1)$, which we check:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  p(1) & = 1 - 1 \\\\ & = 0. \\\\\n",
    "  p'(1) - q(1) & = (3 - 2) - \\frac{1}{12} (23 -16 + 5) \\\\ & = 1 - \\frac{12}{12} \\\\ & = 0.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So the method is consistent.\n",
    "\n",
    "To check stability we have to find the roots of the stability polynomial $p(z)$. We write\n",
    "$$\n",
    "  p(z) = z^2 (z − 1)\n",
    "$$\n",
    "to see that the roots are 0 (twice) and 1, which means that the method satisfies the *strong* root condition implying both stability and relative stability, meaning it is a useful method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the 2-step Adams-Bashforth method to the ODE from Worksheet 4,\n",
    "$$\n",
    "  y' + 2 y = 2 − e^{−4x}, \\quad y(0) = 1.\n",
    "$$\n",
    "Use the Euler or Euler predictor-corrector method to start. Again, find the value of y(1) (analytic answer is $1 − (e^{−2} − e^{−4} )/2)$ and see how your method converges with resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Coding Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbconvert": {
     "hide_solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from matplotlib import pyplot\n",
    "\n",
    "def AB2(f, y0, interval, N = 100, start = 'Euler'):\n",
    "    \"\"\"\n",
    "    Solve the IVP y' = f(x, y) on the given interval using N+1 points\n",
    "    (counting the initial point) with initial data y0.\n",
    "    \"\"\"\n",
    "    \n",
    "    h = (interval[1] - interval[0]) / N\n",
    "    x = numpy.linspace(interval[0], interval[1], N+1)\n",
    "    y = numpy.zeros((len(y0), N+1))\n",
    "    ff = numpy.zeros((len(y0), N+1))\n",
    "    y[:, 0] = y0\n",
    "    ff[:, 0] = f(x[0], y[:, 0])\n",
    "    \n",
    "    if (start == 'Euler'):\n",
    "        y[:, 1] = y0 + h * ff[:, 0]\n",
    "    elif (start == 'Euler PC'):\n",
    "        yp = y0 + h * ff[:, 0]\n",
    "        y[:, 1] = y0 + h * ( ff[:, 0] + f(x[1], yp) ) / 2.0 \n",
    "    else:\n",
    "        raise Exception(\"Only allowed values for start are \"\n",
    "                        \"['Euler', 'Euler PC']\")\n",
    "    ff[:, 1] = f(x[1], y[:, 1])\n",
    "    \n",
    "    for i in range(1, N):\n",
    "        y[:, i+1] = y[:, i] + h * ( 3.0 * ff[:, i] - ff[:, i-1] ) / 2.0\n",
    "        ff[:, i+1] = f(x[i+1], y[:, i+1])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def fn_q1(x, y):\n",
    "    \"\"\"\n",
    "    Function defining the IVP in question 1.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 2.0 - numpy.exp(-4.0*x) - 2.0*y\n",
    "\n",
    "# Now do the test\n",
    "\n",
    "exact_y_end = 1.0 - (numpy.exp(-2.0) - numpy.exp(-4.0)) / 2.0\n",
    "\n",
    "# Test at default resolution\n",
    "x, y = AB2(fn_q1, numpy.array([1.0]), [0.0, 1.0])\n",
    "print(\"Error at the end point is \", y[:, -1] - exact_y_end)\n",
    "\n",
    "fig = pyplot.figure(figsize = (12, 8), dpi = 50)\n",
    "pyplot.plot(x, y[0, :], 'b-+')\n",
    "pyplot.xlabel('$x$', size = 16)\n",
    "pyplot.ylabel('$y$', size = 16)\n",
    "\n",
    "# Now do the convergence test\n",
    "levels = numpy.array(range(4, 10))\n",
    "Npoints = 2**levels\n",
    "abs_err_Euler   = numpy.zeros(len(Npoints))\n",
    "abs_err_EulerPC = numpy.zeros(len(Npoints))\n",
    "for i in range(len(Npoints)):\n",
    "    x, y = AB2(fn_q1, numpy.array([1.0]), [0.0, 1.0], Npoints[i])\n",
    "    abs_err_Euler[i] = abs(y[0, -1] - exact_y_end)\n",
    "    x, y = AB2(fn_q1, numpy.array([1.0]), [0.0, 1.0], Npoints[i],\n",
    "               'Euler PC')\n",
    "    abs_err_EulerPC[i] = abs(y[0, -1] - exact_y_end)\n",
    "\n",
    "# Best fit to the errors\n",
    "h = 1.0 / Npoints\n",
    "p_Euler = numpy.polyfit(numpy.log(h), numpy.log(abs_err_Euler), 1)\n",
    "p_EulerPC = numpy.polyfit(numpy.log(h), numpy.log(abs_err_EulerPC), 1)\n",
    "\n",
    "fig = pyplot.figure(figsize = (12, 8), dpi = 50)\n",
    "pyplot.loglog(h, abs_err_Euler, 'kx')\n",
    "pyplot.loglog(h, numpy.exp(p_Euler[1]) * h**(p_Euler[0]), 'k-')\n",
    "pyplot.loglog(h, abs_err_EulerPC, 'bo')\n",
    "pyplot.loglog(h, numpy.exp(p_EulerPC[1]) * h**(p_EulerPC[0]), 'b--')\n",
    "pyplot.xlabel('$h$', size = 16)\n",
    "pyplot.ylabel('$|$Error$|$', size = 16)\n",
    "pyplot.legend(('AB2 Errors (Euler start)',\n",
    "               \"Best fit line slope {0:.3}\".format(p_Euler[0]),\n",
    "               'AB2 Errors (Euler PC start)',\n",
    "               \"Best fit line slope {0:.3}\".format(p_EulerPC[0])),\n",
    "              loc = \"upper left\")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "Both converge at order two: oddly, the results starting with the Euler predictor-corrector are noticeably worse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the 2-step implicit Adams-Moulton method to the above ODE, using the 2-step Adams-Bashforth method as a predictor. Use the Euler or Euler predictor-corrector method to start. See how your method converges with resolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "### Answer Coding Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "nbconvert": {
     "hide_solution": true
    }
   },
   "outputs": [],
   "source": [
    "def AM2(f, y0, interval, N = 100, start = 'Euler'):\n",
    "    \"\"\"\n",
    "    Solve the IVP y' = f(x, y) on the given interval using N+1 points\n",
    "    (counting the initial point) with initial data y0.\n",
    "    \"\"\"\n",
    "    \n",
    "    h = (interval[1] - interval[0]) / N\n",
    "    x = numpy.linspace(interval[0], interval[1], N+1)\n",
    "    y = numpy.zeros((len(y0), N+1))\n",
    "    ff = numpy.zeros((len(y0), N+1))\n",
    "    y[:, 0] = y0\n",
    "    ff[:, 0] = f(x[0], y[:, 0])\n",
    "    \n",
    "    if (start == 'Euler'):\n",
    "        y[:, 1] = y0 + h * ff[:, 0]\n",
    "    elif (start == 'Euler PC'):\n",
    "        yp = y0 + h * ff[:, 0]\n",
    "        y[:, 1] = y0 + h * ( ff[:, 0] + f(x[1], yp) ) / 2.0 \n",
    "    else:\n",
    "        raise Exception(\"Only allowed values for start are \"\n",
    "                        \"['Euler', 'Euler PC']\")\n",
    "    ff[:, 1] = f(x[1], y[:, 1])\n",
    "    \n",
    "    for i in range(1, N):\n",
    "        # Adams-Bashforth 2 for the predictor step\n",
    "        yp = y[:, i] + h * ( 3.0 * ff[:, i] - ff[:, i-1] ) / 2.0\n",
    "        # Adams-Moulton 2 for the corrector step\n",
    "        y[:, i+1] = h * (ff[:, i] + f(x[i+1], yp)) / 2.0\n",
    "        ff[:, i+1] = f(x[i+1], y[:, i+1])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def fn_q2(x, y):\n",
    "    \"\"\"\n",
    "    Function defining the IVP in question 2.\n",
    "    \"\"\"\n",
    "    \n",
    "    return 2.0 - numpy.exp(-4.0*x) - 2.0*y\n",
    "\n",
    "# Now do the test\n",
    "\n",
    "exact_y_end = 1.0 - (numpy.exp(-2.0) - numpy.exp(-4.0)) / 2.0\n",
    "\n",
    "# Test at default resolution\n",
    "x, y = AB2(fn_q2, numpy.array([1.0]), [0.0, 1.0])\n",
    "print(\"Error at the end point is \", y[:, -1] - exact_y_end)\n",
    "\n",
    "fig = pyplot.figure(figsize = (12, 8), dpi = 50)\n",
    "pyplot.plot(x, y[0, :], 'b-+')\n",
    "pyplot.xlabel('$x$', size = 16)\n",
    "pyplot.ylabel('$y$', size = 16)\n",
    "\n",
    "# Now do the convergence test\n",
    "levels = numpy.array(range(4, 10))\n",
    "Npoints = 2**levels\n",
    "abs_err_AM2_Euler   = numpy.zeros(len(Npoints))\n",
    "abs_err_AM2_EulerPC = numpy.zeros(len(Npoints))\n",
    "for i in range(len(Npoints)):\n",
    "    x, y = AB2(fn_q2, numpy.array([1.0]), [0.0, 1.0], Npoints[i])\n",
    "    abs_err_AM2_Euler[i] = abs(y[0, -1] - exact_y_end)\n",
    "    x, y = AB2(fn_q2, numpy.array([1.0]), [0.0, 1.0], Npoints[i],\n",
    "               'Euler PC')\n",
    "    abs_err_AM2_EulerPC[i] = abs(y[0, -1] - exact_y_end)\n",
    "\n",
    "# Best fit to the errors\n",
    "h = 1.0 / Npoints\n",
    "p_Euler = numpy.polyfit(numpy.log(h), numpy.log(abs_err_AM2_Euler), 1)\n",
    "p_EulerPC = numpy.polyfit(numpy.log(h), numpy.log(abs_err_AM2_EulerPC), 1)\n",
    "\n",
    "fig = pyplot.figure(figsize = (12, 8), dpi = 50)\n",
    "pyplot.loglog(h, abs_err_AM2_Euler, 'kx')\n",
    "pyplot.loglog(h, numpy.exp(p_Euler[1]) * h**(p_Euler[0]), 'k-')\n",
    "pyplot.loglog(h, abs_err_AM2_EulerPC, 'bo')\n",
    "pyplot.loglog(h, numpy.exp(p_EulerPC[1]) * h**(p_EulerPC[0]), 'b--')\n",
    "pyplot.xlabel('$h$', size = 16)\n",
    "pyplot.ylabel('$|$Error$|$', size = 16)\n",
    "pyplot.legend(('AM2 Errors (Euler start)',\n",
    "               \"Best fit line slope {0:.3}\".format(p_Euler[0]),\n",
    "               'AM2 Errors (Euler PC start)',\n",
    "               \"Best fit line slope {0:.3}\".format(p_EulerPC[0])),\n",
    "              loc = \"upper left\")\n",
    "\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbconvert": {
     "hide_solution": true
    }
   },
   "source": [
    "The results are essentially identical to Adams-Bashforth 2."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
